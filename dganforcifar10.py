# -*- coding: utf-8 -*-
"""DGANforCifar10

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GlorP5QWrhheb5DKRk8Mm_ZWXaUR4muh
"""

from keras.layers import Flatten
from keras.layers import Dropout
from numpy import expand_dims
from numpy import zeros
from numpy import ones
from numpy import vstack
from numpy.random import randn
from numpy.random import randint
from keras.datasets.cifar10 import load_data
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Dropout
from matplotlib import pyplot
import numpy as np

(x_train,_),(_,_)=load_data()

def define_discriminator(in_shape=(32,32,3)):
	model = Sequential()
	# normal
	model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))
	model.add(LeakyReLU(alpha=0.2))
	# downsample
	model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))
	model.add(LeakyReLU(alpha=0.2))
	# downsample
	model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))
	model.add(LeakyReLU(alpha=0.2))
	# downsample
	model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))
	model.add(LeakyReLU(alpha=0.2))
	# classifier
	model.add(Flatten())
	model.add(Dropout(0.4))
	model.add(Dense(1, activation='sigmoid'))
	# compile model
	opt = Adam(lr=0.0002, beta_1=0.5)
	model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
	return model
 
# define the standalone generator model
def define_generator(latent_dim):
	model = Sequential()
	# foundation for 4x4 image
	n_nodes = 256 * 4 * 4
	model.add(Dense(n_nodes, input_dim=latent_dim))
	model.add(LeakyReLU(alpha=0.2))
	model.add(Reshape((4, 4, 256)))
	# upsample to 8x8
	model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
	model.add(LeakyReLU(alpha=0.2))
	# upsample to 16x16
	model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
	model.add(LeakyReLU(alpha=0.2))
	# upsample to 32x32
	model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
	model.add(LeakyReLU(alpha=0.2))
	# output layer
	model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))
	return model
 
# define the combined generator and discriminator model, for updating the generator
def define_gan(g_model, d_model):
	# make weights in the discriminator not trainable
	d_model.trainable = False
	# connect them
	model = Sequential()
	# add generator
	model.add(g_model)
	# add the discriminator
	model.add(d_model)
	# compile model
	opt = Adam(lr=0.0002, beta_1=0.5)
	model.compile(loss='binary_crossentropy', optimizer=opt)
	return model

def train_the_model(epochs,no_of_samples_per_batch,latent_points,dis,gen,GAN):
  no_of_batches=x_train.shape[0]//no_of_samples_per_batch
  dis=dis
  gen=gen
  GAN=GAN
  half_batches=no_of_batches//2
  for i in range(epochs):
   print("epoch is :",i)
   for j in range(no_of_batches):
     #create no_of_samples_per_batch//2: fake stuff
     #create no_of_samples_per_batch//2: fake stuff
     z=np.random.randn(latent_points*half_batches)
     z=z.reshape(half_batches,latent_points)
     z=gen.predict(z)
     #create 50 real stuff
     w=np.random.randint(0,x_train.shape[0],half_batches)
     w=x_train[w]
     w=(w-127.5)/127.5
     #create new dataset with fake and real stuff
   
     #create new output to train on:
     yz=np.zeros((half_batches, 1))
     yw=np.ones((half_batches,1))
     #train discriminator
     print("We are on loop",j)
     loss1,acu1=dis.train_on_batch(z,yz)
     print("loss and accuracy on fake data respectively is :",loss1, acu1)
     loss2,accu2=dis.train_on_batch(w,yw)
     print("loss and accuracy on real data respectively is :",loss2, accu2)
     #train gan
     #creating gan latent input
     gan_input_latent=np.random.randn(no_of_batches*latent_points)
     gan_input_latent=gan_input_latent.reshape(no_of_batches,latent_points)
     GAN.train_on_batch(gan_input_latent,np.ones((no_of_batches,1)))

dis=define_discriminator()
gen=define_generator(100)
GAN=define_gan(gen,dis)
train_the_model(1,100,100,dis,gen,GAN)

def save_model(gen):
  gen.save("trained_gen.h5")

save_model(gen)

import tensorflow as tf

def recreate_gen(model_name):
  return tf.keras.models.load_model(model_name)

v=recreate_gen("trained_gen.h5")

def create_images(no_of_images,latent_points):
   q=np.random.randn(latent_points*no_of_images)
   q=q.reshape(no_of_images,latent_points)
   q=gen.predict(q)
   return q

def show_images(image_no,latent_points,no_of_images):
  stack_of_images=create_images(no_of_images,latent_points)
  plt.imshow(stack_of_images[image_no-1])


